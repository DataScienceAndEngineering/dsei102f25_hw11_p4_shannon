{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 11: Shannon's Markov Text Generator\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In his 1948 paper \"A Mathematical Theory of Communication,\" Claude Shannon demonstrated how natural language could be modeled using statistical methods. He showed that by using n-gram statistics (sequences of n words), we could generate text that increasingly resembles natural English as n increases.\n",
    "\n",
    "In this assignment, you will implement Shannon's n-order Markov text generator. The algorithm works as follows:\n",
    "\n",
    "1. **Order 0**: Pick random words from a dictionary (pure randomness)\n",
    "2. **Order 1**: Pick words based on single-word frequency in a corpus\n",
    "3. **Order n**: Pick the next word based on the previous n-1 words\n",
    "\n",
    "### Key Algorithm Components:\n",
    "\n",
    "- **Context matching**: Look for sequences of n-1 words in the corpus\n",
    "- **Wraparound search**: The corpus is treated as circular\n",
    "- **Fallback mechanism**: If no match at order n-1, try n-2, then n-3, etc.\n",
    "- **Random selection**: Start searches from random positions for variety\n",
    "\n",
    "### Learning Objectives:\n",
    "\n",
    "- Understand the foundations of statistical language modeling\n",
    "- Implement a classic algorithm from information theory\n",
    "- See how increasing order creates more coherent text\n",
    "- Practice working with text processing and random generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Setup and Data Loading\n",
    "\n",
    "Import the required libraries and download the corpus and dictionary.\n",
    "\n",
    "**Requirements:**\n",
    "- Import: `random`, `requests`, `re`, and `typing` components\n",
    "- Download Alice in Wonderland from: `https://www.gutenberg.org/files/11/11-0.txt`\n",
    "- Download the word dictionary from: `https://raw.githubusercontent.com/dwyl/english-words/master/words_alpha.txt`\n",
    "- Store the raw text in variables `corpus_text` and `dictionary_words`\n",
    "- Include error handling with fallback data\n",
    "- Set random seed to 42 for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading corpus...\n",
      "✓ Corpus downloaded: 144696 characters\n",
      "\n",
      "Downloading word list...\n",
      "✓ Dictionary loaded: 370105 words\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import requests\n",
    "import re\n",
    "from typing import List, Optional\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Download corpus\n",
    "print(\"Downloading corpus...\")\n",
    "corpus_url = \"https://www.gutenberg.org/files/11/11-0.txt\"\n",
    "try:\n",
    "    response = requests.get(corpus_url)\n",
    "    corpus_text = response.text\n",
    "    print(f\"✓ Corpus downloaded: {len(corpus_text)} characters\")\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading corpus: {e}\")\n",
    "    corpus_text = \"\"\"The quick brown fox jumps over the lazy dog. \n",
    "    The dog was lazy but the fox was quick and brown. \n",
    "    A quick brown fox can jump very high over any lazy dog.\"\"\"\n",
    "    print(\"Using fallback corpus\")\n",
    "\n",
    "# Download word list\n",
    "print(\"\\nDownloading word list...\")\n",
    "wordlist_url = \"https://raw.githubusercontent.com/dwyl/english-words/master/words_alpha.txt\"\n",
    "try:\n",
    "    response = requests.get(wordlist_url)\n",
    "    dictionary_words = [w.strip().lower() for w in response.text.split('\\n') if w.strip()]\n",
    "    print(f\"✓ Dictionary loaded: {len(dictionary_words)} words\")\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading word list: {e}\")\n",
    "    dictionary_words = [\"the\", \"a\", \"an\", \"dog\", \"cat\", \"fox\", \"jumps\", \"runs\", \n",
    "                       \"lazy\", \"quick\", \"brown\", \"over\", \"was\", \"and\", \"but\", \"can\",\n",
    "                       \"zebra\", \"yellow\", \"xylophone\", \"water\", \"violet\"]\n",
    "    print(\"Using fallback dictionary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Text Preprocessing - Tokenization\n",
    "\n",
    "Implement a `tokenize` function that prepares text for the Markov generator.\n",
    "\n",
    "**Requirements:**\n",
    "- Function name: `tokenize(text: str) -> List[str]`\n",
    "- Convert text to lowercase\n",
    "- Remove all punctuation (keep only letters, numbers, and spaces)\n",
    "- Split into individual words\n",
    "- Remove empty strings from the result\n",
    "- Test your function with: \"Hello, World! This is a test.\"\n",
    "\n",
    "**Hint:** Use `re.sub(r'[^\\w\\s]', ' ', text)` to remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Hello, World! This is a test.\n",
      "Tokenized: ['hello', 'world', 'this', 'is', 'a', 'test']\n",
      "✓ Tokenization working correctly\n"
     ]
    }
   ],
   "source": [
    "# Write a simple tokenizer\n",
    "def tokenize(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Tokenize text by converting to lowercase, removing punctuation, and splitting into words.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text string\n",
    "        \n",
    "    Returns:\n",
    "        List of tokenized words\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    raise Exception(\"Not implemented\")\n",
    "    \n",
    "    # Remove punctuation using regex, keeping only word characters and spaces\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    \n",
    "    # Split into words and remove empty strings\n",
    "    raise Exception(\"Not implemented\")\n",
    "    \n",
    "    return words\n",
    "\n",
    "# Test the tokenization function\n",
    "raise Exception(\"Not implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Process the Corpus\n",
    "\n",
    "Apply the tokenization to the downloaded corpus.\n",
    "\n",
    "**Requirements:**\n",
    "- Tokenize `corpus_text` and store in a variable called `corpus`\n",
    "- Print the total number of words\n",
    "- Print the number of unique words\n",
    "- Display the first 30 words\n",
    "- Display a sample from the middle of the corpus (words 1000-1010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus Statistics:\n",
      "  Total words: 27455\n",
      "  Unique words: 2695\n",
      "\n",
      "First 30 words:\n",
      "  start of the project gutenberg ebook 11 illustration alice s adventures in wonderland by lewis carroll the millennium fulcrum edition 3 0 contents chapter i down the rabbit hole chapter\n",
      "\n",
      "Sample from middle (words 1000-1010):\n",
      "  bats eat cats for you see as she couldn t\n"
     ]
    }
   ],
   "source": [
    "# Process the corpus\n",
    "corpus = tokenize(corpus_text)\n",
    "\n",
    "raise Exception(\"Not implemented\")\n",
    "\n",
    "\n",
    "if len(corpus) > 1010:\n",
    "    print(f\"  {' '.join(corpus[1000:1010])}\")\n",
    "else:\n",
    "    print(f\"  Corpus too short, showing last 10 words: {' '.join(corpus[-10:])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Implement Context Matching with Wraparound\n",
    "\n",
    "Implement `find_next_word` that searches for a context in the corpus and returns the following word.\n",
    "\n",
    "**Requirements:**\n",
    "- Function: `find_next_word(corpus: List[str], context: List[str], start_pos: int = None) -> Optional[str]`\n",
    "- Search for the context starting from a random position (or specified start_pos)\n",
    "- **Critical**: Implement wraparound - when you reach the end of the corpus, continue from the beginning\n",
    "- Return the word that follows the context, or None if not found\n",
    "- The search should check ALL positions in the corpus (with wraparound)\n",
    "\n",
    "**Algorithm:**\n",
    "1. Start from random position\n",
    "2. For each position (with wraparound using modulo):\n",
    "   - Check if context matches at this position\n",
    "   - If yes, return the next word (also with wraparound)\n",
    "3. Return None if no match after checking all positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing wraparound with corpus: ['end', 'of', 'text', 'start', 'of', 'text']\n",
      "Word after 'text start': of\n",
      "✓ Wraparound working\n"
     ]
    }
   ],
   "source": [
    "def find_next_word(corpus: List[str], context: List[str], start_pos: int = None) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Find the next word in corpus that follows the given context.\n",
    "    Implements wraparound searching.\n",
    "    \n",
    "    Args:\n",
    "        corpus: List of words in the corpus\n",
    "        context: List of words to match\n",
    "        start_pos: Starting position for search (random if None)\n",
    "        \n",
    "    Returns:\n",
    "        The next word if found, None otherwise\n",
    "    \"\"\"\n",
    "    raise Exception(\"Not implemented\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Test wraparound functionality\n",
    "test_corpus = [\"end\", \"of\", \"text\", \"start\", \"of\", \"text\"]\n",
    "print(\"Testing wraparound with corpus:\", test_corpus)\n",
    "result = find_next_word(test_corpus, [\"text\", \"start\"], start_pos=0)\n",
    "print(f\"Word after 'text start': {result}\")\n",
    "print(f\"✓ Wraparound working\" if result == \"of\" else \"✗ Check wraparound\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Implement the Main Markov Generator\n",
    "\n",
    "Implement the complete `markov_gen` function following Shannon's algorithm.\n",
    "\n",
    "**Function signature:**\n",
    "```python\n",
    "def markov_gen(corpus: List[str], dictionary_words: List[str], \n",
    "               output_len: int = 10, prompt: str = \"\", n_order: int = 2) -> str\n",
    "```\n",
    "\n",
    "**Algorithm Requirements:**\n",
    "\n",
    "1. **Preprocessing:**\n",
    "   - Tokenize the prompt\n",
    "   - Handle edge cases (empty corpus/dictionary)\n",
    "\n",
    "2. **Order 0 (CRITICAL FIX):**\n",
    "   - Pick random words from `dictionary_words` (NOT corpus)\n",
    "   - Use proper random indexing: `dictionary_words[random.randint(0, len(dictionary_words)-1)]`\n",
    "\n",
    "3. **Initialization (if prompt is empty and n_order > 0):**\n",
    "   - Pick a random word from corpus\n",
    "   - Build up context to n_order-1 words\n",
    "\n",
    "4. **Main Generation Loop:**\n",
    "   - Generate exactly `output_len` NEW words\n",
    "   - For each word:\n",
    "     - Try to match context of length n_order-1\n",
    "     - If no match, try n_order-2, then n_order-3, etc. (fallback)\n",
    "     - If no match at any level, pick random word from corpus\n",
    "\n",
    "5. **Return:**\n",
    "   - Original prompt + newly generated words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing basic generation:\n",
      "Generated: the lobster quadrille chapter xii alice s croquet ground chapter i\n",
      "Word count: 11 (should be 11: 1 prompt + 10 new)\n"
     ]
    }
   ],
   "source": [
    "def markov_gen(corpus: List[str], \n",
    "               dictionary_words: List[str], \n",
    "               output_len: int = 10, \n",
    "               prompt: str = \"\", \n",
    "               n_order: int = 2) -> str:\n",
    "    \"\"\"\n",
    "    Generate text using an n-order Markov process following Shannon's algorithm.\n",
    "    \n",
    "    Parameters:\n",
    "    - corpus: list of strings (tokenized words from text corpus)\n",
    "    - dictionary_words: list of strings (vocabulary for n_order=0)\n",
    "    - output_len: int, number of NEW words to generate\n",
    "    - prompt: str, initial text to seed the generation\n",
    "    - n_order: int, order of the Markov chain\n",
    "    \n",
    "    Returns:\n",
    "    - str, prompt + output_len newly generated words\n",
    "    \"\"\"\n",
    "    raise Exception(\"Not implemented\")\n",
    "   \n",
    "    \n",
    "    return ' '.join(sequence)\n",
    "\n",
    "# Test the implementation\n",
    "print(\"Testing basic generation:\")\n",
    "test_result = markov_gen(corpus[:100], dictionary_words[:50], \n",
    "                        output_len=10, prompt=\"the\", n_order=2)\n",
    "print(f\"Generated: {test_result}\")\n",
    "print(f\"Word count: {len(test_result.split())} (should be 11: 1 prompt + 10 new)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Test Order-0 Generation\n",
    "\n",
    "Verify that order-0 generation correctly selects random words from the dictionary.\n",
    "\n",
    "**Requirements:**\n",
    "- Generate 50 words with n_order=0\n",
    "- Verify all words come from the dictionary\n",
    "- Check the distribution of first letters (should be varied, not just 'a')\n",
    "- Generate multiple samples to show randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Order-0 Generation (Random Dictionary Words)\n",
      "==================================================\n",
      "\n",
      "Sample 1:\n",
      "  First 15 words: upjet serapic camatina subtransversally oversoaks apart antimonic castrating flimsilyst gantangs refective talyshin anourous slitters escritoires...\n",
      "  Unique first letters: ['a', 'c', 'd', 'e', 'f', 'g', 'm', 'o', 'p', 'r', 's', 't', 'u', 'w', 'y']\n",
      "  Letter variety: 15 different starting letters\n",
      "  All from dictionary: ✓ Yes\n",
      "\n",
      "Sample 2:\n",
      "  First 15 words: find luteinizing chelem cartilages neuroplasty cenanthy myopathies marrams tarbooshed hydrogenous atoned pneumorrhagia scolophore concrescent neorama...\n",
      "  Unique first letters: ['a', 'b', 'c', 'e', 'f', 'h', 'i', 'l', 'm', 'n', 'p', 's', 't', 'u', 'z']\n",
      "  Letter variety: 15 different starting letters\n",
      "  All from dictionary: ✓ Yes\n",
      "\n",
      "Sample 3:\n",
      "  First 15 words: garamond charros neutercane yardkeep pyriform twelves mohels differency motorization micromesentery farad unslinking hypercyanotic wretchlessly verbalizes...\n",
      "  Unique first letters: ['b', 'c', 'd', 'f', 'g', 'h', 'm', 'n', 'p', 's', 't', 'u', 'v', 'w', 'y']\n",
      "  Letter variety: 15 different starting letters\n",
      "  All from dictionary: ✓ Yes\n",
      "\n",
      "✓ Order-0 generation test complete\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing Order-0 Generation (Random Dictionary Words)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Generate multiple samples\n",
    "for i in range(3):\n",
    "\n",
    "    raise Exception(\"Not implemented\")\n",
    "    # Check distribution of first letters\n",
    "    raise Exception(\"Not implemented\")\n",
    "    \n",
    "    # Verify all words are from dictionary\n",
    "    raise Exception(\"Not implemented\")\n",
    "\n",
    "print(\"\\n✓ Order-0 generation test complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Demonstrate Shannon's Orders\n",
    "\n",
    "Generate text at different orders to show how coherence improves.\n",
    "\n",
    "**Requirements:**\n",
    "- Generate 30 words at each order (0, 1, 2, 3, 4)\n",
    "- Start with empty prompt to see natural generation\n",
    "- Display the results clearly\n",
    "- Comment on the increasing coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shannon's N-Order Approximations\n",
      "============================================================\n",
      "Notice how text becomes more English-like as order increases:\n",
      "\n",
      "\n",
      "Order 0 approximation:\n",
      "  (Pure random words from dictionary)\n",
      "    vervecean legalisms bdellidae fumigations aphicide kludged obscenely hyperoxygenize byproducts fearsomely\n",
      "    soothed kinosternon ferreters unheeded realism nonspecializing unclenches pluricarinate cumbersome hydrophoran\n",
      "    cryptocarpic grave sloyd seashells heterosexually stromatopora panglossic strawen nudely misphrase\n",
      "\n",
      "Order 1 approximation:\n",
      "  (Each word depends on previous word)\n",
      "    room how ornamented as try had she in head one\n",
      "    a written would she can but frying the i do\n",
      "    her it her the the knew be i the the\n",
      "    yet\n",
      "\n",
      "Order 2 approximation:\n",
      "  (Bigrams - two-word patterns)\n",
      "    the gryphon lying down stairs how surprised at this of\n",
      "    it sat up in chorus again où est ma am\n",
      "    to alice said the duchess s tail see this there\n",
      "    was\n",
      "\n",
      "Order 3 approximation:\n",
      "  (Trigrams - three-word patterns)\n",
      "    not looking for the mouse looked at it a little\n",
      "    glass table as before and behind it when a sharp\n",
      "    hiss made her look up in such confusion that she\n",
      "    hardly knew\n",
      "\n",
      "Order 4 approximation:\n",
      "  (4-gram patterns)\n",
      "    _will_ be a queer thing to be sure she had\n",
      "    not gone far before they saw the mock turtle s\n",
      "    story chapter x the lobster quadrille the mock turtle went\n",
      "    on at last\n",
      "\n",
      "============================================================\n",
      "Observation: Higher orders produce more coherent phrases!\n"
     ]
    }
   ],
   "source": [
    "print(\"Shannon's N-Order Approximations\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Notice how text becomes more English-like as order increases:\\n\")\n",
    "\n",
    "for n_order in range(5):\n",
    "    raise Exception(\"Not implemented\")\n",
    "    \n",
    "    # Format output nicely\n",
    "raise Exception(\"Not implemented\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8: Test with Different Prompts\n",
    "\n",
    "Test the generator with various prompts to show context-aware generation.\n",
    "\n",
    "**Requirements:**\n",
    "- Test at least 4 different prompts\n",
    "- Use n_order=2 and n_order=3\n",
    "- Generate 15-20 words for each\n",
    "- Include prompts like: \"alice was\", \"down the rabbit\", \"the queen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context-Aware Generation with Different Prompts\n",
      "============================================================\n",
      "\n",
      "Using n_order=2:\n",
      "----------------------------------------\n",
      "\n",
      "  Prompt: 'alice was'\n",
      "  Generated: [alice was] walking by the queen to turn them they ll fetch it away the pepper that\n",
      "\n",
      "  Prompt: 'down the rabbit'\n",
      "  Generated: [down the rabbit] s not quite natural way into the rabbit she added as a pig head first\n",
      "\n",
      "  Prompt: 'the queen'\n",
      "  Generated: [the queen] to the mock turtle s no room for a secret kept all for the hedgehog\n",
      "\n",
      "  Prompt: 'said the'\n",
      "  Generated: [said the] riddle yet it this sounded best plan it again _before she said alice it might\n",
      "\n",
      "  Prompt: 'in a'\n",
      "  Generated: [in a] strange and say to go to go _there_ again and a game of lullaby to\n",
      "\n",
      "Using n_order=3:\n",
      "----------------------------------------\n",
      "\n",
      "  Prompt: 'alice was'\n",
      "  Generated: [alice was] just beginning to get very tired of sitting by her sister who was peeping anxiously\n",
      "\n",
      "  Prompt: 'down the rabbit'\n",
      "  Generated: [down the rabbit] sends in a game of croquet she was near enough to try the experiment he\n",
      "\n",
      "  Prompt: 'the queen'\n",
      "  Generated: [the queen] s croquet ground in her life and the fan and skurried away into the air\n",
      "\n",
      "  Prompt: 'said the'\n",
      "  Generated: [said the] duchess replied in an offended tone and the queen who had spoken first that s\n",
      "\n",
      "  Prompt: 'in a'\n",
      "  Generated: [in a] hurry a large one but i hadn t time said the mouse looked at poor\n"
     ]
    }
   ],
   "source": [
    "print(\"Context-Aware Generation with Different Prompts\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "prompts = [\n",
    "    \"alice was\",\n",
    "    \"down the rabbit\",\n",
    "    \"the queen\",\n",
    "    \"said the\",\n",
    "    \"in a\"\n",
    "]\n",
    "\n",
    "for n_order in [2, 3]:\n",
    "    print(f\"\\nUsing n_order={n_order}:\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "        \n",
    "        # Highlight the prompt in the output\n",
    "        raise Exception(\"Not implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 9: Performance Analysis\n",
    "\n",
    "Analyze the performance and text quality at different orders.\n",
    "\n",
    "**Requirements:**\n",
    "- Measure generation time for different orders\n",
    "- Calculate vocabulary diversity (unique words / total words)\n",
    "- Generate 100 words for each test\n",
    "- Compare the metrics across orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance and Quality Analysis\n",
      "============================================================\n",
      "\n",
      "Order    Time (s)     Diversity    Unique/100   Top 3 Words                   \n",
      "--------------------------------------------------------------------------------\n",
      "0        0.0000       1.000        100          smutless(1), chancewise(1), boredom(1)\n",
      "1        0.0000       0.723        73           a(7), the(6), i(5)            \n",
      "2        0.1310       0.812        82           the(6), to(4), alice(3)       \n",
      "3        0.4587       0.696        71           a(7), the(6), she(4)          \n",
      "4        0.6252       0.748        77           of(5), it(4), the(4)          \n",
      "\n",
      "Observations:\n",
      "- Order 0 has highest diversity (random selection)\n",
      "- Higher orders tend to repeat common phrases\n",
      "- Generation time increases slightly with order\n"
     ]
    }
   ],
   "source": [
    "print(\"Performance and Quality Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results = []\n",
    "\n",
    "for n_order in range(5):\n",
    "    # Time the generation\n",
    "    raise Exception(\"Not implemented\")\n",
    "    \n",
    "    # Analyze the generated text\n",
    "    raise Exception(\"Not implemented\")\n",
    "    \n",
    "    # Most common words\n",
    "    raise Exception(\"Not implemented\")\n",
    "    \n",
    "# Display results\n",
    "raise Exception(\"Not implemented\")\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(\"- Order 0 has highest diversity (random selection)\")\n",
    "print(\"- Higher orders tend to repeat common phrases\")\n",
    "print(\"- Generation time increases slightly with order\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 10: Edge Cases and Validation\n",
    "\n",
    "Test edge cases to ensure robustness.\n",
    "\n",
    "**Test cases:**\n",
    "1. Empty corpus (should fall back to dictionary)\n",
    "2. Empty dictionary (should fall back to corpus)\n",
    "3. Very high n_order (larger than corpus)\n",
    "4. Long prompt\n",
    "5. Verify exact output length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge Case Testing\n",
      "============================================================\n",
      "\n",
      "1. Empty corpus test:\n",
      "   Result: test aaronic a aa aaa aah\n",
      "   ✓ Handled empty corpus\n",
      "\n",
      "2. Empty dictionary test:\n",
      "   Result: the mat the sat sat\n",
      "   ✓ Handled empty dictionary\n",
      "\n",
      "3. High n_order test:\n",
      "   Result: cat sat on the mat the cat sat on the\n",
      "   ✗ Failed\n",
      "\n",
      "4. Long prompt test:\n",
      "   Prompt words: 9\n",
      "   Total words: 12\n",
      "   Last 5 words: many words project gutenberg ebook\n",
      "   ✓ Handled long prompt\n",
      "\n",
      "5. Output length verification:\n",
      "   Requested: 5, Got: 5 - ✓ Correct\n",
      "   Requested: 10, Got: 10 - ✓ Correct\n",
      "   Requested: 20, Got: 20 - ✓ Correct\n",
      "\n",
      "============================================================\n",
      "Edge case testing complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"Edge Case Testing\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test 1: Empty corpus\n",
    "print(\"\\n1. Empty corpus test:\")\n",
    "result = markov_gen([], dictionary_words[:20], output_len=5, prompt=\"test\", n_order=2)\n",
    "print(f\"   Result: {result}\")\n",
    "print(f\"   ✓ Handled empty corpus\" if len(result.split()) == 6 else \"   ✗ Failed\")\n",
    "\n",
    "# Test 2: Empty dictionary\n",
    "print(\"\\n2. Empty dictionary test:\")\n",
    "mini_corpus = [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "result = markov_gen(mini_corpus, [], output_len=4, prompt=\"the\", n_order=0)\n",
    "print(f\"   Result: {result}\")\n",
    "print(f\"   ✓ Handled empty dictionary\" if len(result.split()) == 5 else \"   ✗ Failed\")\n",
    "\n",
    "# Test 3: n_order > corpus length\n",
    "print(\"\\n3. High n_order test:\")\n",
    "result = markov_gen(mini_corpus, dictionary_words[:20], output_len=5, prompt=\"\", n_order=10)\n",
    "print(f\"   Result: {result}\")\n",
    "print(f\"   ✓ Handled high n_order\" if len(result.split()) == 5 else \"   ✗ Failed\")\n",
    "\n",
    "# Test 4: Long prompt\n",
    "print(\"\\n4. Long prompt test:\")\n",
    "long_prompt = \"this is a very long prompt with many words\"\n",
    "result = markov_gen(corpus[:100], dictionary_words[:20], output_len=3, prompt=long_prompt, n_order=2)\n",
    "words = result.split()\n",
    "print(f\"   Prompt words: {len(long_prompt.split())}\")\n",
    "print(f\"   Total words: {len(words)}\")\n",
    "print(f\"   Last 5 words: {' '.join(words[-5:])}\")\n",
    "print(f\"   ✓ Handled long prompt\" if len(words) == len(long_prompt.split()) + 3 else \"   ✗ Failed\")\n",
    "\n",
    "# Test 5: Exact output length\n",
    "print(\"\\n5. Output length verification:\")\n",
    "for test_len in [5, 10, 20]:\n",
    "    result = markov_gen(corpus, dictionary_words, output_len=test_len, prompt=\"alice\", n_order=2)\n",
    "    actual = len(result.split()) - 1  # Subtract prompt\n",
    "    print(f\"   Requested: {test_len}, Got: {actual} - {'✓ Correct' if actual == test_len else '✗ Wrong'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Edge case testing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 11: Creative Text Generation\n",
    "\n",
    "Use your implementation to generate some creative text examples.\n",
    "\n",
    "**Requirements:**\n",
    "- Generate a \"story\" continuation with high order (n=3 or 4)\n",
    "- Generate a \"nonsense\" paragraph with order 0\n",
    "- Generate a \"dialogue\" starting with \"said alice\"\n",
    "- Format the output nicely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creative Text Generation Showcase\n",
      "============================================================\n",
      "\n",
      "1. Story Continuation (n_order=4):\n",
      "----------------------------------------\n",
      "   once upon a time there were three little sisters the dormouse began\n",
      "   in a great hurry you did said the mock turtle s story\n",
      "   you can t help it she said to herself because of his\n",
      "   great wig the judge by the way was the king and queen\n",
      "   of hearts and i had to\n",
      "\n",
      "2. Nonsense Paragraph (n_order=0):\n",
      "----------------------------------------\n",
      "   behold the albergatrice schalmei conirostral hormogonium autoalkylation basidigitalia signatary inoculum\n",
      "   wiggliest conglutin unaccessibleness puttyroot chemurgical agrapha sporiparity impressional preciouses proassociation\n",
      "   peridiola malpighia elaborating ballate hayrake prill clericals binoxide obituarily quaint\n",
      "   bottu stalked trilocular vinegrower barmbrack degasifier decent smorgasbord isotherms cajones\n",
      "   grobian coexertion\n",
      "\n",
      "3. Alice's Dialogue (n_order=3):\n",
      "----------------------------------------\n",
      "   \"said alice whispered that it was said alice you must manage the best of educations in fact we went to school in the last word with such\"\n",
      "\n",
      "4. Scene Description (n_order=2):\n",
      "----------------------------------------\n",
      "   the garden was an old father william_ said _everybody_ has he\n",
      "   got up and of the hatter he shall have grown up\n",
      "   at your verdict the lobster quadrille the gryphon lying round the\n",
      "   jurymen on the lobsters and\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"Creative Text Generation Showcase\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Story continuation\n",
    "print(\"\\n1. Story Continuation (n_order=4):\")\n",
    "print(\"-\" * 40)\n",
    "story = markov_gen(corpus, dictionary_words, \n",
    "                  output_len=50, prompt=\"once upon a time\", n_order=4)\n",
    "words = story.split()\n",
    "for i in range(0, len(words), 12):\n",
    "    print(\"   \" + ' '.join(words[i:min(i+12, len(words))]))\n",
    "\n",
    "# Nonsense paragraph\n",
    "print(\"\\n2. Nonsense Paragraph (n_order=0):\")\n",
    "print(\"-\" * 40)\n",
    "nonsense = markov_gen(corpus, dictionary_words, \n",
    "                     output_len=40, prompt=\"behold the\", n_order=0)\n",
    "words = nonsense.split()\n",
    "for i in range(0, len(words), 10):\n",
    "    print(\"   \" + ' '.join(words[i:min(i+10, len(words))]))\n",
    "\n",
    "# Dialogue\n",
    "print(\"\\n3. Alice's Dialogue (n_order=3):\")\n",
    "print(\"-\" * 40)\n",
    "dialogue = markov_gen(corpus, dictionary_words, \n",
    "                     output_len=25, prompt=\"said alice\", n_order=3)\n",
    "print(f'   \"{dialogue}\"')\n",
    "\n",
    "# Description\n",
    "print(\"\\n4. Scene Description (n_order=2):\")\n",
    "print(\"-\" * 40)\n",
    "scene = markov_gen(corpus, dictionary_words, \n",
    "                  output_len=35, prompt=\"the garden was\", n_order=2)\n",
    "words = scene.split()\n",
    "for i in range(0, len(words), 11):\n",
    "    print(\"   \" + ' '.join(words[i:min(i+11, len(words))]))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Reflection\n",
    "\n",
    "You have successfully implemented Shannon's Markov text generator!\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Statistical Language Modeling**: This simple algorithm from 1948 demonstrates the fundamental principle behind modern language models - predicting the next token based on context.\n",
    "\n",
    "2. **Order vs Coherence**: As n increases, the generated text becomes more coherent and English-like, but also more repetitive (copying longer sequences from the training data).\n",
    "\n",
    "3. **The Importance of Randomness**: Starting searches from random positions and having random fallbacks creates variety in the output.\n",
    "\n",
    "4. **Wraparound Search**: Treating the corpus as circular ensures we can find all possible matches.\n",
    "\n",
    "5. **Fallback Mechanism**: Systematically reducing the context length ensures the generator always produces output.\n",
    "\n",
    "### Reflection Questions:\n",
    "\n",
    "1. How does this relate to modern Large Language Models?\n",
    "2. What are the limitations of this n-gram approach?\n",
    "3. How might you improve this algorithm?\n",
    "4. What order (n) produces the best balance between coherence and creativity?\n",
    "\n",
    "### Historical Note:\n",
    "\n",
    "Shannon's work laid the foundation for information theory and computational linguistics. This simple Markov chain approach evolved into Hidden Markov Models, then neural language models, and eventually today's transformer-based LLMs. The core idea remains the same: language has statistical patterns that can be learned and generated."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scistackenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
